{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_factory import TextDataset, getDataloaders\n",
    "import seaborn as sns # For class distribution visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from file_utils import *\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import random_split # For custom data-sets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use this version of TextDataset\n",
    "\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, csv_file, max_length):\n",
    "#         self.data = pd.read_csv(csv_file)\n",
    "#         self.data = self.data[self.data['lang_abv'] == 'en'] # Drop non-english rows \n",
    "#         self.max_length = max_length\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     # Geeling: This is the string to word_ids using PA4 vocab, but if pretrained BERT tokenizer works, let's use that instead\n",
    "# #     def __convert_string_to_word_ids(self, string):\n",
    "# #         # Convert a string to a Tensor of word ids.\n",
    "        \n",
    "# #         vocab = self.vocab\n",
    "# #         tokens = nltk.tokenize.word_tokenize(str(string).lower())\n",
    "# #         word_ids = [vocab('<start>')]\n",
    "# #         word_ids.extend([vocab(token) for token in tokens])\n",
    "# #         word_ids.append(vocab('<end>'))\n",
    "# #         word_ids_tensor = torch.Tensor(word_ids)\n",
    "# #         return word_ids_tensor\n",
    "    \n",
    "#     def _tokenize(self, x):\n",
    "#         # This will truncate or pad to self.max_length, such that the return array is self.max_length in length. \n",
    "#         # [UNK] token is 100\n",
    "#         # start token is 101\n",
    "#         # [SEP] token is 102\n",
    "#         # [PAD] token is 0\n",
    "#         # See more here: https://huggingface.co/transformers/model_doc/bert.html?highlight=berttokenizer#berttokenizer \n",
    "#         return self.tokenizer(\n",
    "#                 x,\n",
    "#                 max_length=self.max_length, \n",
    "#                 truncation=True, \n",
    "#                 padding='max_length')\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "        \n",
    "#         prem = self.data.iloc[idx]['premise']\n",
    "#         hypo = self.data.iloc[idx]['hypothesis']\n",
    "        \n",
    "#         # Tokenize with BERT \n",
    "#         # TODO: make use of 'token_type_ids' and 'attention_mask' (attention will disregard the PAD tokens)\n",
    "#         # Currently only uses 'input_ids' which is a tokenized representation similar to \n",
    "#         premise = self._tokenize(prem)['input_ids']\n",
    "#         hypothesis = self._tokenize(hypo)['input_ids']\n",
    "        \n",
    "#         # Convert strings to word ids. NOT USED\n",
    "#         # premise = self.__convert_string_to_word_ids(prem)\n",
    "#         # hypothesis = self.__convert_string_to_word_ids(hypo)\n",
    "        \n",
    "#         return torch.Tensor(premise), torch.Tensor(hypothesis), self.data.iloc[idx]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use this version of getDataloaders()\n",
    "\n",
    "# def getDataloaders(csv_file, config_data):\n",
    "# #     def collate_fn(data):\n",
    "# #         \"\"\"Creates mini-batch tensors from the list of tuples (image, caption)\n",
    "# #         by padding the captions to make them of equal length.\n",
    "# #         We can not use default collate_fn because variable length tensors can't be stacked vertically.\n",
    "# #         We need to pad the captions to make them of equal length so that they can be stacked for creating a mini-batch.\n",
    "# #         Args:\n",
    "# #             data: list of tuple (premises, hypothesis, label).\n",
    "# #                 - premises: torch tensor of shape (?); variable length.\n",
    "# #                 - hypothesis: torch tensor of shape (?); variable length.\n",
    "# #                 - label: 0, 1, or 2\n",
    "# #         Returns:\n",
    "# #             padded_prem: torch tensor of shape (batch_size, prem_padded_length).\n",
    "# #             padded_hypo: torch tensor of shape (batch_size, hypo_padded_length).\n",
    "# #             labels: list; original labels.\n",
    "# #         \"\"\"\n",
    "# #         # Sort a data list by caption length (descending order).\n",
    "# #         # Geeling: don't think this is necessary, but was in the original collate_fn for some reason\n",
    "# #         # data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "# #         # Unpack original data\n",
    "# #         premises, hypothesis, labels = zip(*data)\n",
    "\n",
    "# #         # Pad premises \n",
    "# #         lengths = [len(prem) for prem in premises]\n",
    "# #         padded_prem = torch.zeros(len(premises), max(lengths)).long()\n",
    "\n",
    "# #         for i, prem in enumerate(premises):\n",
    "# #             end = lengths[i]\n",
    "# #             padded_prem[i, :end] = prem[:end]\n",
    "\n",
    "# #         # Pad hypothesis\n",
    "# #         lengths = [len(hyp) for hyp in hypothesis]\n",
    "# #         padded_hypo = torch.zeros(len(hypothesis), max(lengths)).long()\n",
    "\n",
    "# #         for i, hypo in enumerate(hypothesis):\n",
    "# #             end = lengths[i]\n",
    "# #             padded_hypo[i, :end] = hypo[:end]\n",
    "\n",
    "# #         return padded_prem, padded_hypo, labels\n",
    "\n",
    "#     all_data = TextDataset(csv_file, config_data['generation']['max_length'])\n",
    "#     num_train = int(len(all_data) * 0.7)\n",
    "#     num_val = int(len(all_data) * 0.2)\n",
    "#     num_test = int(len(all_data) * 0.1)\n",
    "    \n",
    "#     torch.manual_seed(torch.initial_seed())\n",
    "#     train_dataset, val_dataset, test_dataset = random_split(all_data, (num_train, num_val, num_test))\n",
    "\n",
    "#     train_dataloader = DataLoader(dataset=train_dataset,\n",
    "#                       batch_size=config_data['dataset']['batch_size'],\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=config_data['dataset']['num_workers'],\n",
    "#                       #collate_fn=collate_fn,\n",
    "#                       pin_memory=True) # Pin memory makes it faster to move from CPU to GPU (may look into loading dataset to GPU directly tho)\n",
    "#     val_dataloader = DataLoader(dataset=val_dataset,\n",
    "#                       batch_size=config_data['dataset']['batch_size'],\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=config_data['dataset']['num_workers'],\n",
    "#                       #collate_fn=collate_fn,\n",
    "#                       pin_memory=True)\n",
    "#     test_dataloader = DataLoader(dataset=test_dataset,\n",
    "#                       batch_size=config_data['dataset']['batch_size'],\n",
    "#                       shuffle=False,\n",
    "#                       num_workers=config_data['dataset']['num_workers'],\n",
    "#                       #collate_fn=collate_fn,\n",
    "#                       pin_memory=True)\n",
    "#     return train_dataloader, val_dataloader, test_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config = read_file(\"./config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See that config is loaded properly\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataloaders\n",
    "train_dl, val_dl, test_dl = getDataloaders('./data/train.csv', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out some details of the first batch in test dataloader as a test\n",
    "for i, (prem, hypo, label) in enumerate(test_dl):\n",
    "    if i % 500 == 0: \n",
    "        inputs = prem.to('cpu')\n",
    "        print(inputs.shape) # n_batch_elems, max_length\n",
    "        print(hypo[0]) # First hypothesis\n",
    "        print(len(label)) # number of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT has 2x the vocab size as the COCO dataset\n",
    "tokenizer.vocab_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or put it together and include padding requirements\n",
    "inputs = tokenizer(\"Hello, my dog is cute [PAD] [PAD] [SEP] he hehe [PAD][PAD][PAD] \", max_length=25, \n",
    "                truncation = True,\n",
    "                padding='max_length')\n",
    "inputs # This will be a dictionary with 'input_ids', 'token_type_ids', 'attention_mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing can happen in two steps\n",
    "inputs = tokenizer.tokenize(\"Hello, my dog is cute [PAD] [PAD] [SEP] he hehe [PAD][PAD][PAD] \")\n",
    "for i in tokenizer.convert_tokens_to_ids(inputs):\n",
    "    print(tokenizer._convert_id_to_token(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the training data as all our data (we technically only have train.csv to work with since it has labels)\n",
    "# TextDataset will automatically only select 'en' English rows\n",
    "csv_file = './data/train.csv'\n",
    "all_data = TextDataset(csv_file, 20)\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train, val, test\n",
    "num_train = int(len(all_data) * 0.7)\n",
    "num_val = int(len(all_data) * 0.2)\n",
    "num_test = int(len(all_data) * 0.1)\n",
    "\n",
    "# Make sure to check that your split produces integer vals that add up to the total number in all_data\n",
    "print(num_train+num_val+num_test)\n",
    "\n",
    "# Random split\n",
    "torch.manual_seed(torch.initial_seed())\n",
    "train_dataset, val_dataset, test_dataset = random_split(all_data, (num_train, num_val, num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of each class we have in each dataset\n",
    "def tally_classes(dataset):\n",
    "    # Assumes last element is the class label: premise, hypothesis, label\n",
    "    class_count = {}\n",
    "    for d in dataset: \n",
    "        label = d[2]\n",
    "        if label not in class_count: \n",
    "            class_count[label] = 0\n",
    "        class_count[label] += 1 \n",
    "        \n",
    "    return class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of random split\n",
    "\n",
    "sns.barplot(data = pd.DataFrame.from_dict([tally_classes(train_dataset)]).melt(), x=\"variable\", y=\"value\", hue=\"variable\").set_title('Hypothesis Type Distribution')\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(data = pd.DataFrame.from_dict([tally_classes(val_dataset)]).melt(), x =\"variable\", y=\"value\", hue=\"variable\").set_title('Hypothesis Type Distribution')\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(data = pd.DataFrame.from_dict([tally_classes(test_dataset)]).melt(), x=\"variable\", y=\"value\", hue=\"variable\").set_title('Hypothesis Type Distribution')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
