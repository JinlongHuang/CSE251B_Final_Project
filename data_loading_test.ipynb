{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_factory import TextDataset, getDataloaders\n",
    "import seaborn as sns # For class distribution visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from file_utils import *\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import random_split # For custom data-sets\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_dataset_export = load_dataset('snli', split='train')\n",
    "\n",
    "# Save to dictionary\n",
    "snli_dict = {'premise': snli_dataset_export['premise'], 'hypothesis': snli_dataset_export['hypothesis'], 'lang_abv': ['en']*len(snli_dataset_export), 'label': snli_dataset_export['label']}\n",
    "snli_df = pd.DataFrame(snli_dict)\n",
    "\n",
    "# Save to csv\n",
    "snli_df.to_csv('./data/snli_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Attempt at creating the dataset rather than loading to CSV\n",
    "\n",
    "# # Get the Stanford Natural Language Inference Dataset\n",
    "# snli_dataset = load_dataset('snli', split='train') # Just use the Train, it gives us 550k which is pleanty! \n",
    "\n",
    "# # Tokenize the premises \n",
    "# snli_dataset = snli_dataset.map(lambda e: tokenizer(e['premise'], max_length=max_length, truncation=True, padding='max_length'), batched=True)\n",
    "# # Map the tokenized outputs to prem_id, prem_token_type_ids, and prem_atten_mask\n",
    "# snli_dataset = snli_dataset.map(lambda e: {'prem_id': e['input_ids'], 'prem_token_type_ids': e['token_type_ids'], 'prem_atten_mask': e['attention_mask']})\n",
    "\n",
    "# # Tokenize the hypothesis \n",
    "# snli_dataset = snli_dataset.map(lambda e: tokenizer(e['hypothesis'], max_length=max_length, truncation=True, padding='max_length'), batched=True)\n",
    "# # Map the tokenized outputs to hypo_id, hypo_token_type_ids, and hypo_atten_mask\n",
    "# snli_dataset = snli_dataset.map(lambda e: {'hypo_id': e['input_ids'], 'hypo_token_type_ids': e['token_type_ids'], 'hypo_atten_mask': e['attention_mask']})\n",
    "\n",
    "# # Set format to match TextDataset - returns a dictionary with the column keys\n",
    "# snli_dataset.set_format(type='torch', columns=['prem_id', 'hypo_id', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Statistical anlaysis on SNLI (length of sentences and number of words per sentence)\n",
    "# snli_dataset_2 = load_dataset('snli', split='train')\n",
    "# snli_dataset_2['premise'][0]\n",
    "# dataset_with_length = snli_dataset_2.map(lambda x: {\"length_premise\": len(x[\"premise\"]), \"length_hypothesis\": len(x[\"hypothesis\"])})\n",
    "# dataset_with_length = dataset_with_length.map(lambda x: {'prem_toks': tokenizer.tokenize(x['premise']), 'hypo_toks': tokenizer.tokenize(x['hypothesis'])})\n",
    "# dataset_with_length = dataset_with_length.map(lambda x: {'prem_toks_length': len(x['prem_toks']), 'hypo_toks_length': len(x['hypo_toks'])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning = list(range(0, 250, 5))\n",
    "# plt.hist(dataset_with_length['length_premise'], bins=binning, alpha=0.5)\n",
    "# plt.hist(dataset_with_length['length_hypothesis'], bins=binning, alpha=0.5)\n",
    "# plt.title(\"Premise and Hypothesis lengths from SNLI\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning = list(range(0, 90, 3))\n",
    "# plt.hist(dataset_with_length['prem_toks_length'], bins=binning, alpha=0.5, label=\"prem tok count\")\n",
    "# plt.hist(dataset_with_length['hypo_toks_length'], bins=binning, alpha=0.5, label=\"hypo tok count\")\n",
    "# plt.title(\"Premise and Hypothesis Token Counts from SNLI\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you make changes to dataset_factory you can reload this cell to update without having to restart the kernel\n",
    "import dataset_factory \n",
    "import importlib\n",
    "importlib.reload(dataset_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config = read_file(\"./default.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See that config is loaded properly\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataloader\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dl, val_dl, test_dl = dataset_factory.getDataloaders(config['dataset']['data_file_path'], config['generation']['max_length'], 512, \n",
    "                               config['dataset']['num_workers'], tokenizer, val_split=0.1, test_split=0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out some details of the first batch in test dataloader as a test\n",
    "count = 0\n",
    "for i, (prem, hypo, label) in enumerate(train_dl):\n",
    "    if i % 100 == 0: \n",
    "        count += 1\n",
    "        inputs = prem.to('cpu')\n",
    "        print(inputs.shape) # n_batch_elems, max_length\n",
    "        print(hypo[0]) # First hypothesis\n",
    "        print(len(label)) # number of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT has 2x the vocab size as the COCO dataset\n",
    "tokenizer.vocab_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or put it together and include padding requirements\n",
    "inputs = tokenizer(\"Hello, my dog is cute [PAD] [PAD] [SEP] he hehe [PAD][PAD][PAD] \", max_length=25, \n",
    "                truncation = True,\n",
    "                padding='max_length')\n",
    "inputs # This will be a dictionary with 'input_ids', 'token_type_ids', 'attention_mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing can happen in two steps\n",
    "inputs = tokenizer.tokenize(\"Hello, my dog is cute [PAD] [PAD] [SEP] he hehe [PAD][PAD][PAD] \")\n",
    "for i in tokenizer.convert_tokens_to_ids(inputs):\n",
    "    print(tokenizer._convert_id_to_token(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you make changes to dataset_factory you can reload this cell to update without having to restart the kernel\n",
    "import dataset_factory \n",
    "import importlib\n",
    "importlib.reload(dataset_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = './data/train.csv'\n",
    "csv_data = pd.read_csv(csv_file)\n",
    "csv_data = csv_data[csv_data['lang_abv'] == 'en'] # Drop non-english rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prem_lengths = [len(tokenizer.tokenize(prem)) for prem in csv_data['premise']]\n",
    "hypo_lengths = [len(tokenizer.tokenize(hypo)) for hypo in csv_data['hypothesis']]\n",
    "\n",
    "binning = list(range(0, 90, 3))\n",
    "plt.hist(prem_lengths, bins=binning, alpha=0.5, label=\"prem tok count\")\n",
    "plt.hist(hypo_lengths, bins=binning, alpha=0.5, label=\"hypo tok count\")\n",
    "plt.title(\"Premise and Hypothesis Token Counts from My Dear Watson\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the training data as all our data (we technically only have train.csv to work with since it has labels)\n",
    "# TextDataset will automatically only select 'en' English rows\n",
    "all_data = dataset_factory.TextDataset(csv_file, 20, tokenizer)\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train, val, test\n",
    "num_train = int(len(all_data) * 0.8)\n",
    "num_val = int(len(all_data) * 0.1)\n",
    "num_test = int(len(all_data) * 0.1)\n",
    "\n",
    "# Make sure to check that your split produces integer vals that add up to the total number in all_data\n",
    "print(num_train+num_val+num_test)\n",
    "\n",
    "# Random split\n",
    "torch.manual_seed(torch.initial_seed())\n",
    "train_dataset, val_dataset, test_dataset = random_split(all_data, (num_train, num_val, num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of each class we have in each dataset\n",
    "def tally_classes(dataset):\n",
    "    # Assumes last element is the class label: premise, hypothesis, label\n",
    "    class_count = {}\n",
    "    for d in dataset: \n",
    "        label = d[2]\n",
    "        if label not in class_count: \n",
    "            class_count[label] = 0\n",
    "        class_count[label] += 1 \n",
    "        \n",
    "    return class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of random split\n",
    "\n",
    "sns.barplot(data = pd.DataFrame.from_dict([tally_classes(train_dataset)]).melt(), x=\"variable\", y=\"value\", hue=\"variable\").set_title('Hypothesis Type Distribution')\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(data = pd.DataFrame.from_dict([tally_classes(val_dataset)]).melt(), x =\"variable\", y=\"value\", hue=\"variable\").set_title('Hypothesis Type Distribution')\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(data = pd.DataFrame.from_dict([tally_classes(test_dataset)]).melt(), x=\"variable\", y=\"value\", hue=\"variable\").set_title('Hypothesis Type Distribution')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
